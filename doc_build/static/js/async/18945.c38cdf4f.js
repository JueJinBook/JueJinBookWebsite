"use strict";(self.webpackChunkjue_jin_book_press=self.webpackChunkjue_jin_book_press||[]).push([["18945"],{257142:function(e,n,r){r.r(n),r.d(n,{default:()=>j});var i=r(552676),s=r(740453);let d=r.p+"static/image/1158bbd6e9877d895e59825145e9137b.1575a862.webp",t=r.p+"static/image/a2d7e5cb3c5d039092cb73043f23dd6d.69223157.webp",a=r.p+"static/image/6656e56b27d642bc50babd1d13c7f642.88973a95.webp",o=r.p+"static/image/3411a41aaed925310f61689e78935254.436fc2b6.webp",c=r.p+"static/image/78355cf86c7743cec669acf76d6fb765.0f521ddf.webp",h=r.p+"static/image/ee38b5f691c45922c4635a958b1f4aad.79a981bf.webp",l=r.p+"static/image/9b3c31f4f2d4853cbc15700301ce5310.ad4e0cfc.webp";function x(e){let n=Object.assign({p:"p",h1:"h1",a:"a",img:"img",strong:"strong",h2:"h2",h3:"h3",code:"code",blockquote:"blockquote",em:"em",ul:"ul",li:"li"},(0,s.ah)(),e.components);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"第 5 节中，我们介绍了注意力机制的工作原理，它是目前最流行的神经网络模型的灵魂机制。而相应的包裹注意力机制的实体形式，就是 Transformer 模型结构组件，我们本节重点介绍一下 Transformer。"}),"\n",(0,i.jsxs)(n.h1,{id:"6chatgpt-的组件transformer-模型结构",children:["6.ChatGPT 的组件：Transformer 模型结构",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#6chatgpt-的组件transformer-模型结构",children:"#"})]}),"\n",(0,i.jsx)(n.p,{children:"Transformer 结构组件主要包括如下模块。"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)("img",{src:l,alt:""})}),"\n",(0,i.jsx)(n.p,{children:"其中，$$(x_1,x_2, ..., x_i, ..., x_n)$$就是上一节中输入的 token embedding，它是一个$$(l_{token} \\times l_{embedding})$$大小的矩阵，其中，$$l_{token}$$是输入文本的 token 个数，GPT3 中这个长度是 4097，意味着输入给 GPT3 模型的最长输入文本的 token 数总共可以有 4097 个，而$$l_{embedding}$$是每一个 token 的 embedding 维度，GPT3 中这个长度是 12280。"}),"\n",(0,i.jsx)(n.p,{children:"而图中深蓝色的 attention 模块就是上一节中介绍的 Self-Attention，它是 Transformer 模型的核心操作。除此之外，还包括扩展参数模块 Feed-Forward 层，而在每一层的前后又包括了浅蓝色 norm 层和 dropout 层。在模型的最后，还包括输出结果层 linear 层，softmax 即用于计算交叉熵的损失函数层（第 8 节介绍）。"}),"\n",(0,i.jsxs)(n.p,{children:["在 GPT3 模型以及此后的各种模型中，Self-Attention 使用的并非如第 5 节所介绍的原始自注意力结构，而是一种稀疏的注意力结构，又称",(0,i.jsx)(n.strong,{children:"稀疏 Transformer（Sparse Transformer）"})," 。"]}),"\n",(0,i.jsx)(n.p,{children:"对于以上模型结构，我们一一展开介绍。"}),"\n",(0,i.jsxs)(n.h2,{id:"稀疏-transformer",children:["稀疏 Transformer",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#稀疏-transformer",children:"#"})]}),"\n",(0,i.jsxs)(n.h3,{id:"稀疏-transfomer-的思想基础",children:["稀疏 Transfomer 的思想基础",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#稀疏-transfomer-的思想基础",children:"#"})]}),"\n",(0,i.jsx)(n.p,{children:"Sparse Transformer 的核心实际上是 Sparse Self-Attention （稀疏自注意力机制）。我们依然使用第 5 节中的例子来介绍："}),"\n",(0,i.jsxs)(n.p,{children:["请补全这条语句：",(0,i.jsx)(n.strong,{children:"掘金"}),"社区是一个便捷的技术交流______"]}),"\n",(0,i.jsxs)(n.p,{children:["在这条文本中，想要补全最终的语句，应当参考前文的信息，而前文总共 14 个字，对空格处影响最大的是",(0,i.jsx)(n.code,{children:"掘金"}),"两个字，而像形容词",(0,i.jsx)(n.code,{children:"便捷的"}),"，系词",(0,i.jsx)(n.code,{children:"是一个"}),"都不是最关键的影响因素。换句话说，我们应当设计一种注意力机制，让模型能够在输出空格字符的时候，最大限度地注意到",(0,i.jsx)(n.code,{children:"掘金"}),"两个字。"]}),"\n",(0,i.jsx)(n.p,{children:"如果我们根据原始的 Self-Attention 计算，假设得到的注意力权重计算结果如下形式："}),"\n",(0,i.jsx)(n.p,{children:"$$Attention\xa0=\xa0(0.371, 0.167, 0.174, 0.236, 0, 0, 0, 0.19, 0,0,0.23,0,0, 0)$$"}),"\n",(0,i.jsxs)(n.p,{children:["在这个注意力权重向量中，共有 14 个元素值，加和为 1，每一项对应输入的一个字符。可以看到，",(0,i.jsx)(n.code,{children:"掘金社区"}),"四个字对应的权重最高，而中间很多的字符实际上对后续填补什么字影响极其微小。其实，计算这些不重要的 token 的注意力权重完全是可以规避掉的。"]}),"\n",(0,i.jsx)(n.p,{children:"另一方面，Self-Attention 的计算公式为："}),"\n",(0,i.jsx)(n.p,{children:"$$Attention(Q,K,V)=softmax(\\cfrac{QK^T}{\\sqrt{d_q}})V$$"}),"\n",(0,i.jsx)(n.p,{children:"这是一个矩阵乘法，其中涉及矩阵乘法计算$$QK^T$$。在 ChatGPT 这样的模型中，矩阵的维度非常大，这个计算量也相当大，时间复杂度在$$o(n^2)$$，其中$$n$$是矩阵维度。如果可以避免计算某些不重要位置的注意力权重值，那么这个计算量会小很多，减少计算耗时，提高模型的计算效率。"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"稀疏 Transformer 的本质，就是选择不计算某些 token 位置的注意力值"}),"。"]}),"\n",(0,i.jsxs)(n.h3,{id:"稀疏-transformer-原理",children:["稀疏 Transformer 原理",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#稀疏-transformer-原理",children:"#"})]}),"\n",(0,i.jsxs)(n.p,{children:["所谓稀疏（Sparse），就是不计算某些位置 token 的注意力权重值，保留想要计算的 token 位置。那么，我们可以事先定义一组 token 的位置的索引，$$S=(S_1,S_2, ..., S_i, ..., S_n)$$，这些是我们想要计算的若干个 token 位置。例如，我们要计算第 14 个位置时，保留的索引位置包括$$S_{14}=(0,1,2,3,5,7,10,13)$$。对应了",(0,i.jsx)(n.code,{children:"掘、金、社、区、一、便、技、流"}),"这几个字。"]}),"\n",(0,i.jsx)(n.p,{children:"接下来，我们计算时就可以充分考虑待计算的索引位置，忽略不计算的索引位置。"}),"\n",(0,i.jsx)(n.p,{children:"$$K_{S_i}\xa0=\xa0(W_Kx_j)_{j\xa0\\in\xa0S_i}$$"}),"\n",(0,i.jsx)(n.p,{children:"在这个公式中，其中内部 $$W_Kx_j$$表示注意力机制中 K 的转换运算，而这里表示运算过程中，仅抽取那些在索引中的位置的 embedding 进行计算。这样一来，矩阵计算量就变少了。同理，对于 $$V$$矩阵也有相同的操作："}),"\n",(0,i.jsx)(n.p,{children:"$$V_{S_i}\xa0=\xa0(W_Vx_j)_{j\xa0\\in\xa0S_i}$$"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"那么，$$Q$$ 矩阵可不可以减少呢？答案是不行的。"}),"\n",(0,i.jsx)(n.p,{children:"注意：$$Q$$代表了要为哪些位置计算注意力权重，显然，我们应当为每一个位置 token 计算权重。然而，在为每一个位置计算注意力时，需要考察哪些位置，这就是$$K$$的含义，这个位置则是可以稀疏的。"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)("img",{src:h,alt:"6-1.png"})}),"\n",(0,i.jsxs)(n.p,{children:["上图中共有 7 个 token 位置。其中，对于 $$K$$ 和 $$V$$，分别只计算了第 1 和 3 个 token 的向量，其它位置的向量被忽略。这样就减少了计算量，仅抽取了部分的位置进行关注。由此，我们就得到了计算某一位置注意力权重的公式：\n$$attention(i)=softmax(\\cfrac{(W_qx_i)K^{T}",(0,i.jsx)(n.em,{children:"{S_i}}{\\sqrt{d}})V"}),"{S_i}$$。"]}),"\n",(0,i.jsx)(n.p,{children:"这个式子就是上图的公式化表示，而把所有位置全部合并起来，就形成了完整的稀疏 Attention 计算公式：\n$$SparseAttention(X,S)=(attention(i))_{i \\in {1,2,...,n}}$$。"}),"\n",(0,i.jsx)(n.p,{children:"在整个计算流程中，我们都在假设，已经事先确定了要保留哪些位置的索引 $$S=(S_1,S_2, ..., S_i, ..., S_n)$$，用于计算注意力权重。那么，到底如何确定要保留哪些位置呢？"}),"\n",(0,i.jsxs)(n.p,{children:["如前图中展示，保留的第 1 和 第 3 个 token 作为索引，其它位置则被丢弃了。一般来讲有两种常用的做法，",(0,i.jsx)(n.strong,{children:"跨步分解注意力机制（Strided Factorized Attention）"})," ",(0,i.jsx)(n.strong,{children:"和"})," ",(0,i.jsx)(n.strong,{children:"固定分解注意力机制（Fixed Factorized Attention）"})," 。它们的原理如下图所示。"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)("img",{src:c,alt:""})}),"\n",(0,i.jsxs)(n.p,{children:["针对这幅图，我们首先看最左边这幅图，",(0,i.jsx)(n.strong,{children:"每次观察一行"}),"。深蓝色小方块代表当前要计算注意力的 token 位置，灰色部分的方块是指被忽略掉的 token，采用的是 Mask 方式（第 7 节中介绍，这里我们仅需知道这些 token 被忽略掉了，只关注蓝色方块即可）。而在每一个深蓝色方块的左侧代表了它在计算注意力时需要参考的 token 位置（浅蓝色），在传统 Transformer 中，每次计算都要参考左侧的所有位置。"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["例：",(0,i.jsx)(n.strong,{children:"掘金"}),"社区是一个便捷的技术交流______"]}),"\n",(0,i.jsxs)(n.p,{children:["传统 Transformer 实际上当计算空格 token 时，需要把前面的每一个字符",(0,i.jsx)(n.code,{children:"掘金社区是一个便捷的技术交流"}),"全部考虑进去。"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"而在中间这幅图，它代表了跨步注意力机制，根据规则，当每次计算深蓝色方块的注意力时，首先需要纳入左侧临近的若干 token，然后在距离当前 token 较远位置的 token 每隔 3 个值参考一个。"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["例：",(0,i.jsx)(n.strong,{children:"掘金"}),"社区是一个便捷的技术交流______"]}),"\n",(0,i.jsxs)(n.p,{children:["跨步分解注意力机制中，实际上当计算空格 token 时，只需要计算字符",(0,i.jsx)(n.code,{children:"社、个、技、术、交、流"}),"，以此预测",(0,i.jsx)(n.code,{children:"平"}),"字。"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)("img",{src:o,alt:"6-2 (1).png"})}),"\n",(0,i.jsx)(n.p,{children:"右侧的图中，不论计算哪一个位置，都需要观察一些固定的位置（浅蓝色）。"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["例：",(0,i.jsx)(n.strong,{children:"掘金"}),"社区是一个便捷的技术交流______"]}),"\n",(0,i.jsxs)(n.p,{children:["固定分解注意力机制中，实际上当计算空格 token 时，只需要计算字符",(0,i.jsx)(n.code,{children:"区、便、术、交、流"}),"。"]}),"\n",(0,i.jsx)(n.p,{children:"读者可以参照上图，自行确定这些字符的索引位置，以加深理解。"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["需要说明的是，在选择哪些位置可以稀疏计算时，选择的方式方法完全可以自定义，有非常大的灵活性。上述的方法存在局限性，尤其是固定注意力机制。不论计算哪些位置的注意力，都默认忽略掉一些位置，很容易造成计算结果的错误（如上述例子中，最关键的",(0,i.jsx)(n.code,{children:"掘金"}),"被忽略了）。所以，在 Transformer 模型中，又加入了多头注意力机制，用于综合多次 Attention 计算的结果。"]}),"\n",(0,i.jsxs)(n.h2,{id:"多头multi-head注意力机制",children:["多头（multi-head）注意力机制",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#多头multi-head注意力机制",children:"#"})]}),"\n",(0,i.jsx)(n.p,{children:"前面讲述了制作一次注意力机制的全过程，所谓多头注意力机制，就是在模型中多做几次注意力机制，以期让模型能够注意到不同的信息。"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)("img",{src:a,alt:"6-3.png"})}),"\n",(0,i.jsx)(n.p,{children:"如图所示，整个过程十分简单，设计了 N 次的注意力机制，把 N 次的结果拼接起来，就完成了输出。所谓拼接方式，如图中不同色度的绿色方块所示，假设两个注意力头计算的结果分别为$$a=[a_0,a_1,...,a_m]$$，$$b=[b_0,b_1,...,b_n]$$，那么拼接后的结果就是$$concat=[a_0,a_1,...,a_m,b_0,...,b_n]$$。"}),"\n",(0,i.jsx)(n.p,{children:"注意，图中仅仅展示了某一位置的 token 的计算结果，实际上的输入为所有 token embedding 构成的矩阵。"}),"\n",(0,i.jsxs)(n.h2,{id:"normalization-正规化",children:["Normalization 正规化",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#normalization-正规化",children:"#"})]}),"\n",(0,i.jsx)(n.p,{children:"如前 Transformer 结构图所示，在每一个 Attention 模块接入之前，都有一个 norm 模块。它是神经网络模型中常见的 Normalization 正规化模块。这里我们还是举一个例子来说明情况。"}),"\n",(0,i.jsx)(n.p,{children:"假设我们在输入稀疏 Self-Attention 模块之前，分别有 2 组具体的 token embedding 值， 维度都是 $$4\xa0\\times\xa03$$，其中每一行代表一个 token 的 embedding 值，具体值情况如图所示。"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)("img",{src:t,alt:"6-4.png"})}),"\n",(0,i.jsx)(n.p,{children:"我们观察上图表，可以看出，左侧表格矩阵中，所有元素的取值都介于 $$(-0.25, 0.25)$$ 之间，取值较为集中，分布较为均匀、稳定，而右侧表格矩阵中，所有元素取值都介于 $$(-3.05, 3.05)$$ 之间，取值波动较大，而且第二行中的 token embedding 分布又介于 $$(-0.2, 0.2)$$ 之间，取值波动又比较小，总体来看，分布不稳定，差异较大。"}),"\n",(0,i.jsx)(n.p,{children:"在神经网络模型的训练中，数据分布较为集中且均匀的话，有助于模型训练的快速推进和收敛，达到模型训练目的。相反，若数据分布分散，则模型训练困难，很容易过拟合。对于 ChatGPT 这样的大规模语言模型，尤其需要注意训练过程中数据的均匀分布，以此加速模型的稳定训练。"}),"\n",(0,i.jsx)(n.p,{children:"因此，在稀疏 Self-Attention 层之前，加入 norm 层，就是把分布松散的数据，整合为分布较为集中的数据。具体方法就是，假设我们待处理数据为 $$x=(x_1,x_2, ..., x_i, ..., x_n)$$，首先计算其均值和方差："}),"\n",(0,i.jsx)(n.p,{children:"$$\\mu =\xa0\\cfrac{1}{n}\\sum x_i$$"}),"\n",(0,i.jsx)(n.p,{children:"$$\\sigma^2 =\xa0\\cfrac{1}{n}\\sum (x_i\xa0-\xa0\\mu)^2$$"}),"\n",(0,i.jsx)(n.p,{children:"然后，针对每一个值，都做一遍正规化（Normalization），embedding 就会都局限于一个稳定的分布内，方便模型的训练。"}),"\n",(0,i.jsx)(n.p,{children:"$$\\hat x_i =\xa0\\cfrac{(x_i\xa0-\xa0\\mu)}{\\sigma}$$"}),"\n",(0,i.jsx)(n.p,{children:"由于这部分知识都是初中数学，这里就不再举例介绍。事实上，Normalization 一般分为 Batch Normalization、Layer Normalization 等。在 NLP 领域，常用的是 Layer Normalization，它就是指，针对每一条输入数据的所有数值元素做 norm 操作。"}),"\n",(0,i.jsxs)(n.h2,{id:"dropout-机制",children:["Dropout 机制",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#dropout-机制",children:"#"})]}),"\n",(0,i.jsx)(n.p,{children:"在 Transformer 结构图所示，在每一个 Attention 模块接入之前，都有一个 dropout 模块。这个模块的主要功能是防止模型在训练过程中的过拟合。"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"过拟合（"})," ",(0,i.jsx)(n.strong,{children:"O"})," ",(0,i.jsx)(n.strong,{children:"verfitting）"})," ，是指模型在训练数据上表现良好，但在测试数据上表现较差的现象。简单来说，过拟合就是模型过于复杂，以至于在训练数据上表现得非常好，但在新数据上的泛化能力却很差。过拟合通常是由于模型过于复杂，或者训练数据过少导致的。当模型过于复杂时，它会尝试去适应训练数据中的每一个细节，甚至是噪声，导致在新数据上的表现不佳。而当训练数据过少时，模型可能无法学习到足够的特征，也会导致过拟合。"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"比如，一家公司宣布自己研发的 XX 大模型能够作诗、解数学题、翻译、文摘，以及回答各种各样的问题，也晒出了一些数据截图，表明自己的 XX 模型效果优异。"}),"\n",(0,i.jsx)(n.p,{children:"开放公测后，用户试用了一下，发现 XX 模型效果非常差，经常答非所问，给出错误答案。这就是典型的模型过拟合的表现。"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"因此，一个直观的解决模型过拟合的办法就是对模型进行简化。放在 Transformer 中，就是对某些计算结果进行随机地置 0 操作。具体来讲，假设 Attention 层输出了一组 token 矩阵，模型经过一个完全随机的 dropout 之后，其结果中的某些元素就被置为 0。"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)("img",{src:d,alt:"6-5.png"})}),"\n",(0,i.jsx)(n.p,{children:"这样，就好像模型中的一些元素特征被人为屏蔽掉了，从而模型得到了简化。"}),"\n",(0,i.jsx)(n.p,{children:"在模型中，dropout 置 0 是随机进行的，但到底要把多少元素值置 0，需要预先人为设置一个概率值 dropout ratio，当这个值是 0 时，相当于所有元素都不置 0。在上图中，大致可以计算得到，dropout 率为$$5/12=0.417$$。"}),"\n",(0,i.jsxs)(n.h2,{id:"resnet-残差模块",children:["ResNet 残差模块",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#resnet-残差模块",children:"#"})]}),"\n",(0,i.jsxs)(n.p,{children:["在 Transformer 结构图中，Attention 模块的输入 embed 和输出结果有一个叠加，这种叠加操作被称为残差模块。之所以这么操作，主要是为了方便模型的训练过程中，",(0,i.jsx)(n.strong,{children:"梯度不会消失或爆炸"}),"。其本质目的在于顺利使模型完成训练，达到目标效果。具体细节会在第 8 节模型训练过程中做介绍。"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)("img",{src:"data:image/webp;base64,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",alt:""})}),"\n",(0,i.jsxs)(n.h2,{id:"linear-feed-forward-全连接层",children:["Linear Feed-forward 全连接层",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#linear-feed-forward-全连接层",children:"#"})]}),"\n",(0,i.jsx)(n.p,{children:"全连接层，就像在之前内容中为 Q、K、V 添加参数，实现了一个模型参数的扩增。针对一个 token 的输入$$x_i$$，需要做一个矩阵乘法运算，公式表示其输出为$$y_i=Wx_i$$，其中 $$W$$ 就是要学习的参数。这个步骤就是 Linear Feed-forward 层，中文名又叫线性全连接层，核心即矩阵乘法运算。这一步的主要作用在于为模型增加参数，增强模型的拟合能力。"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Dropout 用于控制模型参数过多，过于复杂；而这里线性全连接层又在扩增参数，是否矛盾？"})}),"\n",(0,i.jsxs)(n.p,{children:["其实并不矛盾。增加线性全连接层，目标在于",(0,i.jsx)(n.strong,{children:"提升模型拟合能力的上限"}),"。而在这个很高的上限内，模型的训练浮动比较大，Dropout 主要用于控制模型能够顺利训练到位。"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"至此，本节介绍完了 Transformer 结构组件。把 Transformer 堆叠起来，就形成了 GPT 模型的雏形，这一部分在第7节介绍。"}),"\n",(0,i.jsxs)(n.h1,{id:"总结",children:["总结",(0,i.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#总结",children:"#"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Transformer 组件的核心结构就是 Self-Attention，组件的堆叠构成了 ChatGPT 的语言模型。"}),"\n",(0,i.jsx)(n.li,{children:"自从 GPT3 模型之后（也包括 ChatGPT），使用的是 Sparse Transformer，它有助于减轻模型的计算量，加速模型的训练和使用。"}),"\n",(0,i.jsx)(n.li,{children:"Transformer 中用到了 Normalization、残差计算、线性层，以增强模型的拟合能力、适应性。"}),"\n"]})]})}function p(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,s.ah)(),e.components);return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(x,{...e})}):x(e)}let j=p;p.__RSPRESS_PAGE_META={},p.__RSPRESS_PAGE_META["%E4%BA%BA%E4%BA%BA%E9%83%BD%E8%83%BD%E7%9C%8B%E6%87%82%E7%9A%84%20ChatGPT%20%E5%8E%9F%E7%90%86%E8%AF%BE%2F6.ChatGPT%20%E7%9A%84%E7%BB%84%E4%BB%B6%EF%BC%9ATransformer%20%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.md"]={toc:[{text:"稀疏 Transformer",id:"稀疏-transformer",depth:2},{text:"稀疏 Transfomer 的思想基础",id:"稀疏-transfomer-的思想基础",depth:3},{text:"稀疏 Transformer 原理",id:"稀疏-transformer-原理",depth:3},{text:"多头（multi-head）注意力机制",id:"多头multi-head注意力机制",depth:2},{text:"Normalization 正规化",id:"normalization-正规化",depth:2},{text:"Dropout 机制",id:"dropout-机制",depth:2},{text:"ResNet 残差模块",id:"resnet-残差模块",depth:2},{text:"Linear Feed-forward 全连接层",id:"linear-feed-forward-全连接层",depth:2}],title:"总结",headingTitle:"总结",frontmatter:{}}}}]);