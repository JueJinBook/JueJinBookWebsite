"use strict";(self.webpackChunkjue_jin_book_press=self.webpackChunkjue_jin_book_press||[]).push([["66499"],{532851:function(e,n,r){r.r(n),r.d(n,{default:()=>f});var s=r(552676),d=r(740453);let c=r.p+"static/image/68474348e15adbbd2dfbca6541348bf3.12a905c2.webp",h=r.p+"static/image/0766375409b711f6626aacef4be2a94c.f70a68b3.webp",a=r.p+"static/image/a35f1fbea01dc330be0cd7d53ec3a2bd.daabbbbc.webp",i=r.p+"static/image/9784ecf9eccc0b8dcd57db4e9f0f5faa.2b89610b.webp",t=r.p+"static/image/1044203a0d2d9b42fed5ff0043b6242a.f364bdd7.webp",o=r.p+"static/image/351b4e7bb4dac0576b9fd950cefdf64a.8dd42b22.webp",l=r.p+"static/image/b3384cd5dc89afead602fb51924efa43.418456dd.webp",x=r.p+"static/image/62e0d64420a2fa76b574a11a79ae71f4.c6a2ef75.webp",j=r.p+"static/image/4c9e50a7eb21467461ce446a03ea9d45.435a9f5f.webp",p=r.p+"static/image/ef45a2357aaf3f3addbe1fbf064aba4c.8fac5b5a.webp";function $(e){let n=Object.assign({p:"p",h1:"h1",a:"a",img:"img",strong:"strong",ul:"ul",li:"li",blockquote:"blockquote",h2:"h2",code:"code",h3:"h3",table:"table",thead:"thead",tr:"tr",th:"th"},(0,d.ah)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"前面第 5-6 节介绍了 Self-Attention 自注意力机制、Transformer 模型结构。这就相当于我们盖房子准备好了砖头，本节主要介绍如何把 Tranformer 模型组合起来，形成一个完整的 GPT 模型结构，而组合的方式，就用到了Encoder-Decoder 编解码架构模式。在这一节中，我们将对前述章节的内容做一个汇总，让读者对 GPT 模型从全局有一个清晰的认知。"}),"\n",(0,s.jsxs)(n.h1,{id:"7chatgpt-的结构encoder-decoder",children:["7.ChatGPT 的结构：Encoder-Decoder",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#7chatgpt-的结构encoder-decoder",children:"#"})]}),"\n",(0,s.jsx)(n.p,{children:"在第 1-2 节中，我们大致介绍了语言模型的编解码结构。如下图所示。"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:p,alt:"7-1.png"})}),"\n",(0,s.jsxs)(n.p,{children:["实际上，encoder-decoder 这一套模型架构最早是用于解决机器翻译问题的，感兴趣的读者可以读一下这篇经典论文【2014：",(0,s.jsx)(n.a,{href:"https://scholar.google.com/scholar?q=2014+-+Neural+Machine+Translation+by+Jointly+Learning+to+Align+and+Translate&hl=en&as_sdt=0&as_vis=1&oi=scholart",target:"_blank",rel:"noopener noreferrer",children:"Neural Machine Translation by Jointly Learning to Align and Translate"}),"】。机器翻译模型接收一条英文语句，然后经过模型的一番操作，最后输出一条对应的中文翻译结果。这种建模最早被称为 seq2seq，其含义为 sequence to sequence，即序列到序列，输入一条文字序列，输出一条文字序列。"]}),"\n",(0,s.jsx)(n.p,{children:"此后，在 NLP 领域，seq2seq 可以被应用于各种各样的文字序列任务上，也就成了 NLP 领域的一种标准建模方式。"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:j,alt:"7-2.png"})}),"\n",(0,s.jsx)(n.p,{children:"在 encoder-decoder 这种建模方式中，encoder 像极了一个人接收文字信息思考的过程，decoder 则像极了一个人将大脑中的信息转换成语言表达出来的过程。可以说，encoder-decoder 就是一种机器模拟人脑思考的方式。"}),"\n",(0,s.jsxs)(n.p,{children:["另外，encoder-decoder 并非指一种具体的模型结构，它是一种宽泛的模型",(0,s.jsx)(n.strong,{children:"设计架构"}),"。"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"这种编解码结构不仅仅局限于 NLP 领域，它也可以应用于图像处理、音频处理等领域，例如图像领域的对抗生成模型 GAN 等。"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"我们知道，GPT 采用了 decoder 架构，丢弃了其中的 encoder 部分，其中的具体结构是 Transformer。但是，编解码架构的内部设计还可以采用循环神经网络（ RNN ）这种模型结构。当然，如果未来有更好的设计，也可以替换为别的具体模型结构。"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"如果说 GPT 是一幢房子，Transformer 是盖房子的砖头，那么，encoder-decoder 模型架构就是房子的具体结构，如门朝哪开，有几间卧室等等。除了使用 Transformer 这种砖头之外，还可以使用 RNN 等木头来搭建同样结构的房子。"}),"\n",(0,s.jsx)(n.p,{children:"当然，随着大模型的发展，实践证明，Transformer 这种结构，不论从计算性能还是适用性来说，都比 RNN 模型要强。"}),"\n"]}),"\n",(0,s.jsxs)(n.h2,{id:"gpt-中的编解码架构",children:["GPT 中的编解码架构",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#gpt-中的编解码架构",children:"#"})]}),"\n",(0,s.jsx)(n.p,{children:"接下来，让我们来绘制一下，Transformer 是如何嵌入 GPT 的 encoder-decoder 架构中的。如下图所示，Transformer 模型结构中省略了 norm 正规化、残差计算和 dropout 模块。"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:x,alt:"7-3.png"})}),"\n",(0,s.jsxs)(n.p,{children:["当用户输入了",(0,s.jsx)(n.code,{children:"掘金社区是一个便捷的技术交流"}),"这条语句时，模型首先结合第 4 节中介绍的 embedding 词嵌入，将文本转换为 token，进而找出对应的 token embedding 和 position embedding（本例中不需要 segment embedding），将两者相加即可进入 Transformer 结构做注意力计算。"]}),"\n",(0,s.jsx)(n.p,{children:"Transformer 结构本身可以有很多层，每一层的输入 tensor 和输出 tensor 维度大小全部相同，前一层 Transformer 的输出就可以作为下一层的输入，像罗列方块积木一样。直到最后一层。如下图所示，这里省略了 Transformer 内部的结构，展示了三层 Transformer 结构。"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:l,alt:"7-4.png"})}),"\n",(0,s.jsxs)(n.p,{children:["假设最后一层的 Transformer 输出了 $$M\xa0\\times\xa0N$$ 维的 tensor，如上图中的紫色部分，其中$$M$$表示 token 的个数，$$N$$表示每个 token 的维度，同时假设在 BPE 算法的 token 词表中，总共有 $$K$$个 token。那么，最后一层线性层可以设计为$$M\xa0\\times\xa0N\xa0\\to\xa0K$$的一个函数映射，得到一个 $$K$$ 维向量，如上图中的黄色部分。一般来讲，这个向量的维度可能达到几万维到几十万维不等，",(0,s.jsx)(n.strong,{children:"它的长度和词表中有多少 token 是相等"}),"的。"]}),"\n",(0,s.jsxs)(n.p,{children:["对这个 $$K$$ 维向量进行",(0,s.jsx)(n.strong,{children:"解码"}),"，就可以得到输出的结果。我们来具体讲讲如何解码。"]}),"\n",(0,s.jsxs)(n.h2,{id:"解码得到输出-token",children:["解码得到输出 Token",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#解码得到输出-token",children:"#"})]}),"\n",(0,s.jsxs)(n.h3,{id:"贪婪搜索-greedy-search",children:["贪婪搜索 Greedy Search",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#贪婪搜索-greedy-search",children:"#"})]}),"\n",(0,s.jsxs)(n.p,{children:["假设，我们根据 BPE 算法（第 4 节）得到一份词表，按顺序，其中第 6 个 token 为 ",(0,s.jsx)(n.code,{children:"平"})," 字，第 8 个 token 为",(0,s.jsx)(n.code,{children:"网"}),"字。"]}),"\n",(0,s.jsxs)(n.table,{children:["\n",(0,s.jsxs)(n.thead,{children:["\n",(0,s.jsxs)(n.tr,{children:["\n",(0,s.jsx)(n.th,{children:"1: 我"}),"\n",(0,s.jsx)(n.th,{children:"2：中"}),"\n",(0,s.jsx)(n.th,{children:"3：上"}),"\n",(0,s.jsx)(n.th,{children:"4：这"}),"\n",(0,s.jsx)(n.th,{children:"5：天"}),"\n",(0,s.jsxs)(n.th,{children:["6：",(0,s.jsx)(n.code,{children:"平"})]}),"\n",(0,s.jsx)(n.th,{children:"7：司"}),"\n",(0,s.jsxs)(n.th,{children:["8：",(0,s.jsx)(n.code,{children:"网"})]}),"\n",(0,s.jsx)(n.th,{children:"..."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"根据第 5 节中介绍的 softmax 函数，可以对计算的 $$K$$ 维向量做一个 softmax，得到一个长达 $$K$$ 维的概率分布，假设具体数值如下所示："}),"\n",(0,s.jsxs)(n.table,{children:["\n",(0,s.jsxs)(n.thead,{children:["\n",(0,s.jsxs)(n.tr,{children:["\n",(0,s.jsx)(n.th,{children:"0.0016"}),"\n",(0,s.jsx)(n.th,{children:"0.1120"}),"\n",(0,s.jsx)(n.th,{children:"0.0011"}),"\n",(0,s.jsx)(n.th,{children:"0.00006"}),"\n",(0,s.jsx)(n.th,{children:"0.0015"}),"\n",(0,s.jsx)(n.th,{children:(0,s.jsx)(n.code,{children:"0.3410"})}),"\n",(0,s.jsx)(n.th,{children:"0.0102"}),"\n",(0,s.jsx)(n.th,{children:(0,s.jsx)(n.code,{children:"0.2179"})}),"\n",(0,s.jsx)(n.th,{children:"..."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["从中可以看到，概率值最大的一个数字是 0.3410，它是这个 $$K$$ 维向量的第 6 个数字。因此，我们就可以从 token 词表中选择第 6 个 token，即 ",(0,s.jsx)(n.code,{children:"平"})," 字作为模型输出的结果，整体句子就成了 ",(0,s.jsx)(n.code,{children:"掘金社区是一个便捷的技术交流平"}),"。这种根据向量最大值寻找对应索引的操作叫做 $$argmax$$。",(0,s.jsx)(n.strong,{children:"按照最大概率值选择模型输出的 token"}),"，这种方法叫做",(0,s.jsx)(n.strong,{children:"贪婪搜索（Greedy Search）"})," 。"]}),"\n",(0,s.jsx)(n.p,{children:"既然我们得到了一个维度长达 $$K$$ 维的向量，所有向量值相加为 1，那么我们可以对 token 词表进行采样。"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["所谓",(0,s.jsx)(n.strong,{children:"采样"}),"，简单理解就是掷骰子。我们都知道，一颗方形骰子有 6 个面，分别代表 1，2，3，4，5，6 几种选择。每次投掷，得到的结果是一次采样，每次的投掷结果均不同，每一种结果命中率都是六分之一。"]}),"\n",(0,s.jsx)(n.p,{children:"而在 ChatGPT 模型输出结果时，也以上述采样的方式，按照每个 token 对应的命中概率值进行随机抽取，只不过，可选择范围包含了 token 词表中所有的 token。这就说明了模型输出的结果具有随机性，并非每次都相同。"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["按照贪婪搜索的方式，实际上是取消了根据 token 概率分布做采样的操作。我们知道，有 0.3410 的概率输出得到",(0,s.jsx)(n.code,{children:"平"}),"字，这个结果没错；但也有 0.2179 的概率模型会输出得到",(0,s.jsx)(n.code,{children:"网"}),"字，这个值也很高，若模型输出 ",(0,s.jsx)(n.code,{children:"网"}),"字，语言读起来完全能说得通，并不能算作错。"]}),"\n",(0,s.jsx)(n.p,{children:"因此，贪婪搜索是有一定缺陷的，即人为地漏掉了一些正确的答案。"}),"\n",(0,s.jsxs)(n.h3,{id:"束搜索-beam-search",children:["束搜索 Beam Search",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#束搜索-beam-search",children:"#"})]}),"\n",(0,s.jsx)(n.p,{children:"为了克服贪婪搜索会漏掉一些概率值稍低的正确答案这个缺陷，可以预先选择一个范围。比如，我们把采样范围扩大，选择结果中概率值最大的两个索引位置，即 0.3410 和 0.2179，其余的概率值全部不考虑。仅从这两个概率对应的 token 中进行筛选，那选择 token 的概率分别为："}),"\n",(0,s.jsx)(n.p,{children:"$$p(平)=\\cfrac{0.3410}{0.3410 + 0.2179}=0.61$$"}),"\n",(0,s.jsx)(n.p,{children:"$$p(网)=\\cfrac{0.2179}{0.3410 + 0.2179}=0.39$$"}),"\n",(0,s.jsxs)(n.p,{children:["由此，我们可以从这套概率分布中二选一，0.61 的概率可以抽取出 ",(0,s.jsx)(n.code,{children:"平"})," token，0.39 的概率可以抽取出 ",(0,s.jsx)(n.code,{children:"网"})," token。"]}),"\n",(0,s.jsxs)(n.p,{children:["然后，我们可以继续迭代，让模型输出",(0,s.jsx)(n.code,{children:"平台"}),"二字，或",(0,s.jsx)(n.code,{children:"网站"}),"二字，这两种答案都是正确的。这种先选择最高概率的若干选项，再在其中随机抽取的方式叫做",(0,s.jsx)(n.strong,{children:"束搜索（Beam Search）"})," ，所谓 Beam 就是指一束光中，光线不止一条，而是有多条，对应在解码中，就是指有多个选择，它可以克服贪婪搜索选择范围仅仅只有 1 个 token 的缺点。"]}),"\n",(0,s.jsx)(n.p,{children:"要选择多少个备选 token 进行采样，需要预先人为设定一个数量 $$k$$。可以看出，若此值越小，生成的结果越固定，反之结果越灵活多变。"}),"\n",(0,s.jsxs)(n.h3,{id:"核搜索-nucleus-search",children:["核搜索 Nucleus Search",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#核搜索-nucleus-search",children:"#"})]}),"\n",(0,s.jsx)(n.p,{children:"在 Beam Search 中，选择多少个可选 token 也有一定的策略，例如，上面我们设定了只选择概率值最大的 2 个值。或者换个思路，我们可以设定，把模型输出的 $$K$$ 维概率分布值按从大到小的方式排列，若前 T 个概率值加起来大于 top_p（介于 0~1 之间的值），则以这 T 个值作为最终的抽取范围。"}),"\n",(0,s.jsx)(n.p,{children:"以上面为例，假设预先设定$$top_p=0.6$$，而分布中，概率值最大的三个相加可得：\n$$0.3410 + 0.2179 + 0.1120=0.6709\xa0>\xa00.6$$。"}),"\n",(0,s.jsx)(n.p,{children:"因此，我们就从第 2，6，8 三个 token 中，按照概率进行 token 抽取。"}),"\n",(0,s.jsxs)(n.p,{children:["这种输出方式被称为 ",(0,s.jsx)(n.strong,{children:"top_p 搜索，也叫核搜索（Nucleus Search）"})," ，是 Beam Search 的一种变体。当预先设定的 $$top_p$$ 值变大时，可用于选择的 token 数就变多，模型生成的结果就更加多变，不可靠；当 $$top_p$$ 变小时，可选择的 token 数就变少，模型生成的结果就更加固定；当 $$top_p$$ 为 0 时，核搜索退化为 Greedy Search。"]}),"\n",(0,s.jsxs)(n.p,{children:["在这个例子中，",(0,s.jsx)(n.code,{children:"平"})," token 的概率值是 0.3410，",(0,s.jsx)(n.code,{children:"网"}),"token的概率值是 0.2179，剩下的 token 占据了很大的概率值，但就不再是正确答案了。这实际上也存在一定的缺陷，即正确目标的概率值尽管是最大的，但是数值本身仍然偏小。"]}),"\n",(0,s.jsxs)(n.h3,{id:"温控搜索-temperature-search",children:["温控搜索 Temperature Search",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#温控搜索-temperature-search",children:"#"})]}),"\n",(0,s.jsxs)(n.p,{children:["面对上面的问题，我们得设计一种思路，能够在保证各个 token 的概率值相对大小排序不变的情况下，调整其概率值。例如，",(0,s.jsx)(n.code,{children:"平"})," token 的概率值比",(0,s.jsx)(n.code,{children:"网"}),"大，这个大小关系是确定的，但希望",(0,s.jsx)(n.code,{children:"平"})," 的概率值变得更大一些。"]}),"\n",(0,s.jsx)(n.p,{children:"回忆一下第 5 节中对 softmax 函数的介绍，在上图中，softmax 函数的输入是一个 logits，输出是对应的每个 token 位置的概率值。这里的 logits 就可以理解为对数化的概率值，设 $$u$$ 是 logits 向量，$$p$$ 是对应的概率值，那么两者之间的关系为：$$p_i=\\cfrac{exp(u_i)}{\\sum_j exp(u_j)}$$。"}),"\n",(0,s.jsxs)(n.p,{children:["这就是 softmax 公式。所谓",(0,s.jsx)(n.strong,{children:"温控搜索（Temperature Search）"})," ，就是对上述公式做一个调整，加入一个参数 $$T\xa0\\in\xa0(0,1)$$，得到：$$p_i=\\cfrac{exp(\\cfrac{u_i}{T})}{\\sum_j exp(\\cfrac{u_j}{T})}$$。"]}),"\n",(0,s.jsxs)(n.p,{children:["以",(0,s.jsx)(n.code,{children:"平"}),"字为例，当$$T=1$$时，其对应的概率值是 0.3410，而当 T 值逐渐变小后，其对应的概率值会逐渐变大，模型对",(0,s.jsx)(n.code,{children:"平"}),"字抽取的概率也就变大，模型的输出结果也就更加固定。相反，T 值逐渐趋近于 1 时，模型搜索结果也就变得更加灵活多变，不可控制。"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"这种现象非常像物理热力学中的现象，当温度升高后，分子热运动加剧，运动变得无规则不可控；而当温度降低后，分子热运动减弱，从宏观上看，运动趋于稳定。"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"ChatGPT 的解码方法，就是温控搜索和核搜索的一种结合体。一方面，从全局 token 中选择一个范围（核搜索），另一方面，条件这个范围内的温控参数（温控参数），使得采样不同 token 的值出现变化。如果读者尝试过调用 openai 的接口，就会看到这个参数选项值。"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:o,alt:""})}),"\n",(0,s.jsx)(n.p,{children:"最后，需要提一下，模型解码是个循环过程，即根据前一个 token，输出后一个 token，反复可以迭代进行。那么，这个循环什么时候被打破？换句话说，模型什么时候输出完呢？"}),"\n",(0,s.jsxs)(n.p,{children:["其实，在 token 词表中，单独设置了一个特殊符号 token",(0,s.jsx)(n.code,{children:"<eos>"}),"，意指 end of speech，当模型预测输出了这个 token，那么循环就终端了，ChatGPT 也就会认为，整个要输出的答案完成了。"]}),"\n",(0,s.jsxs)(n.h2,{id:"gpt-是一个解码器",children:["GPT 是一个解码器",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#gpt-是一个解码器",children:"#"})]}),"\n",(0,s.jsx)(n.p,{children:"从上面的整个计算流程中我们可以看到，模型首先接收一串输入数据，经过 Transformer，这属于 encoder 部分，末尾模型输出预测的 token 字符，这属于 decoder 部分。整个流程和本节第一个流程示意图中略有差别。主要差别在于，前述介绍的编解码架构中，编码器和解码器相互独立，中间有一个单独的信号相连。而在 GPT 模型中，实际上编解码融为一体。"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:t,alt:"7-5.png"})}),"\n",(0,s.jsxs)(n.p,{children:["下图是 Transformer 论文中【2017 - ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/1706.03762.pdf",target:"_blank",rel:"noopener noreferrer",children:"Attention Is All You Need"}),"】的最原始的编解码模型架构，其中，左半边为编码器，右半边为解码器。而前述的 GPT 模型架构，其实就是原始结构的右半边。因此，我们也常说，GPT 系列模型仅仅使用了 decoder 部分。"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:i,alt:""})}),"\n",(0,s.jsx)(n.p,{children:"另外，读者可以对比上图最原始的 Transformer 的内部结构和第 6 节中 GPT 结构的异同。如果仍不清楚，可以返回上一节再阅读一下。提示一下，主要包括稀疏操作、norm 操作等等。"}),"\n",(0,s.jsxs)(n.h1,{id:"mask-掩码层",children:["Mask 掩码层",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#mask-掩码层",children:"#"})]}),"\n",(0,s.jsxs)(n.p,{children:["在实际的模型计算过程中，读者可能已经使用过 ChatGPT，模型输入和输出的句子有长有短，长则几千个字，短的不足 10 个字，始终处于变动。而 ChatGPT 模型结构则相对固定，会预先设置允许一个模型接受的固定最大 token 序列长度。假设模型可以接收的最大 token 数量为 20 个，而用户输入的数据只有 ",(0,s.jsx)(n.code,{children:"掘金社区是一个便捷的技术交流"}),"这 14 个 token 时，模型会对这部分数据做一定处理，如下图所示。"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:a,alt:"7-6.png"})}),"\n",(0,s.jsxs)(n.p,{children:["图中展示了模型输入 token embedding 的整体结构，它规定了模型的输入最大 token 数为 20。当用户的输入低于 20 个 token 时，模型会自动为用户的输入补齐，添加一个特殊的",(0,s.jsx)(n.code,{children:"<pad>"})," token（图中以",(0,s.jsx)(n.code,{children:"/p"}),"表示，它和",(0,s.jsx)(n.code,{children:"<eos>"}),"token 性质是相同的，并不是表示具体的某个字符，而是仅仅起到一种功能上的作用），它本身不具有实际意义，仅代表占位符；当用户的输入超过 20 个 token 时，模型又会将超出的部分截断，仅保留前 20 个 token，形成一个不完整的输入。"]}),"\n",(0,s.jsxs)(n.p,{children:["模型在进行后续的 Transformer 操作时，需要考虑",(0,s.jsx)(n.code,{children:"<pad>"})," token 本身不代表任何含义，这些字符不应该参与上下文的相关性自注意力计算。为了解决这个问题，",(0,s.jsx)(n.strong,{children:"Mask 掩码层"}),"被提出。"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Mask 广义来讲是一块蒙版，指覆盖在图像上，遮挡部分景物。"}),"\n",(0,s.jsx)(n.p,{children:"如下图中，用一块白色的蒙版，遮住图片中的部分内容，就只能看见猫咪的一只眼睛。"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:h,alt:""})}),"\n",(0,s.jsxs)(n.p,{children:["Mask 概念同样可以运用在 Transformer 中。根据第 5 节的介绍，自注意力机制主要采用$$Q, K, V$$三种向量进行计算。如果考虑 Mask 蒙版，假设 ChatGPT 模型仅输入了",(0,s.jsx)(n.code,{children:"掘金"}),"两个 token 字符，预测接下来应该填写的字符，则它将变为如下形式："]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)("img",{src:c,alt:"7-7.png"})}),"\n",(0,s.jsx)(n.p,{children:"在上图中，mask 层为每一个 token 位置设置了一个 1，0 取值，若设为 1，则对应位置 token 正常进行计算，而设为 0 时，对应位置不参与注意力计算。最终的 softmax 概率值为 0。"}),"\n",(0,s.jsx)(n.p,{children:"根据第 6 节中的介绍，ChatGPT 采用了稀疏自注意力机制，根据跨步分解（Strided Factorized）和固定分解（Fixed Factorized）两种方式屏蔽掉某些 token 位置的注意力计算，采取的也是 Mask 的方式。"}),"\n",(0,s.jsxs)(n.h1,{id:"总结",children:["总结",(0,s.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#总结",children:"#"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ChatGPT 模型基于 encoder-decoder 模型架构进行建模。"}),"\n",(0,s.jsxs)(n.li,{children:["ChatGPT 模型采用",(0,s.jsx)(n.strong,{children:"核搜索、温控搜索"}),"结合的方式生成输出结果，并基于 temperature 调节生成结果的随机性，值越大，随机性越强，值越小，生成的内容越固定。"]}),"\n",(0,s.jsxs)(n.li,{children:["ChatGPT 主要采用 ",(0,s.jsx)(n.strong,{children:"Mask 掩码"}),"的方式，屏蔽掉不参与注意力计算的 token 位置。"]}),"\n"]})]})}function m(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,d.ah)(),e.components);return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)($,{...e})}):$(e)}let f=m;m.__RSPRESS_PAGE_META={},m.__RSPRESS_PAGE_META["%E4%BA%BA%E4%BA%BA%E9%83%BD%E8%83%BD%E7%9C%8B%E6%87%82%E7%9A%84%20ChatGPT%20%E5%8E%9F%E7%90%86%E8%AF%BE%2F7.ChatGPT%20%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%9AEncoder-Decoder.md"]={toc:[{text:"GPT 中的编解码架构",id:"gpt-中的编解码架构",depth:2},{text:"解码得到输出 Token",id:"解码得到输出-token",depth:2},{text:"贪婪搜索 Greedy Search",id:"贪婪搜索-greedy-search",depth:3},{text:"束搜索 Beam Search",id:"束搜索-beam-search",depth:3},{text:"核搜索 Nucleus Search",id:"核搜索-nucleus-search",depth:3},{text:"温控搜索 Temperature Search",id:"温控搜索-temperature-search",depth:3},{text:"GPT 是一个解码器",id:"gpt-是一个解码器",depth:2}],title:"总结",headingTitle:"总结",frontmatter:{}}}}]);