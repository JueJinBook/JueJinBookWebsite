"use strict";(self.webpackChunkjue_jin_book_press=self.webpackChunkjue_jin_book_press||[]).push([["75559"],{545881:function(n,e,r){r.r(e),r.d(e,{default:()=>x});var s=r(552676),i=r(740453);let h=r.p+"static/image/c2412ad93cf877dea86f37b13b42989b.eefd5050.webp",d=r.p+"static/image/8527969696fe0ce03c40ada562a4fbb5.2fe84ef1.webp",l=r.p+"static/image/f40b601af0649c8b1488c436e768ffee.aa30aaf2.webp",c=r.p+"static/image/09e4013ca6fb894bdf63c76b675240f9.98ef7a80.webp";function a(n){let e=Object.assign({p:"p",h1:"h1",a:"a",strong:"strong",img:"img",h2:"h2",ul:"ul",li:"li",blockquote:"blockquote",h3:"h3"},(0,i.ah)(),n.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.p,{children:"强化学习上一次为大众熟知，还是 2017 年围棋人工智能模型 AlphaGO 打败柯洁的时候。AlphaGo 是由 Google DeepMind 开发的人工智能程序，它使用了深度强化学习算法，能够通过自我学习和对弈经验不断提高自己的水平，它充分展现了强化学习的效果和能力。而 ChatGPT 则将强化学习引入了 NLP 领域，展现出类似人的智能效果。"}),"\n",(0,s.jsx)(e.p,{children:"本节主要简单介绍一下强化学习的基本概念，以及它在 NLP 中的建模情况，为学习 RLHF 方法做一个铺垫。"}),"\n",(0,s.jsxs)(e.h1,{id:"10模型训练基础什么是强化学习",children:["10.模型训练基础：什么是强化学习？",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#10模型训练基础什么是强化学习",children:"#"})]}),"\n",(0,s.jsxs)(e.p,{children:["强化学习是一种机器学习方法，旨在让",(0,s.jsx)(e.strong,{children:"智能体（Agent，即人工智能模型）通过与环境的交互来学习如何做出最优决策（Policy）。在强化学习中，智能体根据所处的环境（Environment）中的状态（State），通过执行动作（Action）来影响环境，并从环境中获得奖励（Reward）或惩罚"}),"。智能体的目标是通过学习最大化长期奖励来制定最佳策略。"]}),"\n",(0,s.jsx)(e.p,{children:"强化学习非常像生物的进化，通过不断地突变基因，由环境来筛选，进而适应环境，生存下来。强化学习的应用非常广泛，主要有游戏、自然语言处理等领域。"}),"\n",(0,s.jsxs)(e.p,{children:["它的基本建模图如下所示。在超级玛丽奥游戏中，马里奥主人公就是一个由人或模型操控的智能体，游戏的每一个关卡，就是强化学习中的环境。我们玩马里奥游戏的过程，实际上就是一个强化学习的最好例子：从不会玩游戏，通过一次次不断地尝试、失败，最后练成一个马里奥游戏高手，成功通关。这说明了强化学习的本质，是一种 Trial & Failed 学习模式，用中文表达，就是",(0,s.jsx)(e.strong,{children:"屡战屡败，屡败屡战，总结经验，获得成功"}),"。用一句俗话来概括，强化学习就是“吃一堑，长一智”。"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)("img",{src:c,alt:""})}),"\n",(0,s.jsxs)(e.h2,{id:"强化学习要素",children:["强化学习要素",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#强化学习要素",children:"#"})]}),"\n",(0,s.jsx)(e.p,{children:"接下来，我们借助马里奥的例子来解释一下强化学习的几大要素。"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"状态（State）：环境和智能体共同构成的整体状态，这个状态与时间有关，因为每时每刻，状态会变动，智能体可能会改变，环境也可能会变。一般用字母 s 来表示状态，所有时刻的状态构成了一个集合，$$s\xa0\\in\xa0S$$ 这是一个有限状态的集合。"}),"\n"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"在超级马里奥游戏中，玩家每时每刻所处的位置、以及游戏画面中的各种物体和危险障碍都属于游戏的状态。"}),"\n",(0,s.jsx)(e.p,{children:"在围棋中，棋盘上对弈双方棋子的分布情况，就是围棋当前的状态。"}),"\n"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"动作（Action）：智能体可以做出的动作描述，所有的动作构成了一个集合，$$a\xa0\\in\xa0A$$ 是一个有限动作集合。"}),"\n"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"在超级马里奥游戏中，玩家可以通过手柄上下左右的方向键，以及射击和跳跃键，总共 6 个按键来控制马里奥的动作。"}),"\n",(0,s.jsx)(e.p,{children:"在围棋中，对弈的一方可以把棋子下在棋盘某个位置。围棋棋盘的大小为 19*19，因此，智能体可选择的动作范围最多就是 361。"}),"\n"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"策略（Policy）：环境的感知状态到行动的映射方式，$$\\pi(s)\xa0\\to a$$，其含义为，根据当前状态，设计决策函数，采取某种动作。"}),"\n"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"在超级马里奥游戏中，玩家观察到前方有金币（状态），所以按下跳跃键（动作）获取金币，这就是一种游戏操作策略。"}),"\n",(0,s.jsx)(e.p,{children:"在围棋中，对弈的一方观察棋盘情况（状态），决定把棋子下在哪个位置（动作）。"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)("img",{src:l,alt:"10-1.png"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"反馈、奖励（Reward）：环境对智能体行动的反馈。在根据当前一个状态做出决策后，智能体将处于下一个状态，并得到一个反馈值。"}),"\n"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"在超级马里奥游戏中，玩家操作马里奥刚吃完金币（当前状态），按下前进键（动作），刚好撞上前方的乌龟（下一个状态），游戏失败（反馈）就是游戏环境对玩家的一种反馈，也就是惩罚（负奖励）。"}),"\n",(0,s.jsx)(e.p,{children:"在围棋中，对弈的一方把棋子下在一个正确的位置（动作），直接制胜棋局（反馈），就是围棋对棋手的奖励反馈。"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)("img",{src:d,alt:"10-2.png"})}),"\n",(0,s.jsxs)(e.h2,{id:"一条强化学习路径",children:["一条强化学习路径",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#一条强化学习路径",children:"#"})]}),"\n",(0,s.jsxs)(e.p,{children:["现在，我们可以得到一条",(0,s.jsx)(e.strong,{children:"状态-策略-反馈"}),"的路径："]}),"\n",(0,s.jsx)(e.p,{children:"$$s_0\xa0\\to a_0\xa0\\to\xa0r_0\xa0\\to\xa0s_1\xa0\\to a_1\xa0\\to\xa0r_1\xa0\\to\xa0...\xa0\\to\xa0s_t\xa0\\to a_t\xa0\\to\xa0r_t$$"}),"\n",(0,s.jsxs)(e.p,{children:["这样一条",(0,s.jsx)(e.strong,{children:"路径"}),"也被称作一次",(0,s.jsx)(e.strong,{children:"采样"}),"，或一条",(0,s.jsx)(e.strong,{children:"轨迹"}),"。"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"以马里奥游戏为例，这条路径实际上就是玩家玩游戏的全过程："}),"\n",(0,s.jsx)(e.p,{children:"平坦的路 => 按前进键 => 前方有乌龟 => 按射击键 => 前方有陷阱 => 按跳跃键 => 营救公主 => 胜利"}),"\n",(0,s.jsx)(e.p,{children:"每一次玩游戏都是一次强化学习的试验，在数学上可以看作是一次强化学习的路径采样。在这个例子中，reward 反馈就是玩家是否营救到公主，或者遇到乌龟挂掉。所以，需要经历一条完整的游戏过程才能确定 reward 值。假设玩家成功通关记为 1，失败记为 0，对应到上述标准路径中，每一次的 reward 值都是相同的，都是 1。"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:["继续观察这样一条路径，当我们在 $$s$$ 状态时，做下一次动作决策，实际上所依赖的经验完全基于当前的状态。这种只依赖当前状态的特性，就叫做",(0,s.jsx)(e.strong,{children:"马尔可夫性"}),"。"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"以马里奥游戏为例，当玩家决定按下跳跃键，以此跳跃陷阱时，之前游戏路径上是否有乌龟，对后续的状态和决策是毫无影响的。"}),"\n"]}),"\n",(0,s.jsxs)(e.h2,{id:"价值函数",children:["价值函数",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#价值函数",children:"#"})]}),"\n",(0,s.jsx)(e.p,{children:"根据前面的基本概念定义，我们可以得到：一个智能体需要不断更新自己的策略函数，以期达到最优的效果。那么如何定义这个效果呢？这里就需要用到价值函数。"}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"例如，在一盘围棋中，我执黑棋在某个位置下了一颗子，这个子对于我是否能赢得棋局胜利，究竟有何影响，主要体现在接下来的棋局状况之中。"}),"\n",(0,s.jsx)(e.p,{children:"换句话说，一条策略的价值，需要衡量它对接下来的操作步骤的影响，设计价值函数也就需要关心策略对未来操作的影响。"}),"\n"]}),"\n",(0,s.jsxs)(e.h3,{id:"状态价值函数",children:["状态价值函数",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#状态价值函数",children:"#"})]}),"\n",(0,s.jsx)(e.p,{children:"状态价值函数（State Value Function）：该函数是针对一个策略 $$\\pi$$ 而言的，它是指从状态 $$s$$ 出发，遵循策略 $$\\pi$$ 能够获得的期望回报。"}),"\n",(0,s.jsx)(e.p,{children:"$$V^{\\pi}(s)=E_{\\pi}[G_t|S_t=s]$$"}),"\n",(0,s.jsx)(e.p,{children:"其中："}),"\n",(0,s.jsx)(e.p,{children:"$$G_t\xa0=\xa0R_t\xa0+\xa0\\gamma R_{t+1}\xa0+\xa0\\gamma^2 R_{t+2}\xa0+\xa0...\xa0+\xa0\\gamma^T R_{t+T}\xa0=\xa0\\sum^T_0\xa0\\gamma^kR_{t+T}$$"}),"\n",(0,s.jsx)(e.p,{children:"首先，状态价值函数 $$V^{\\pi}(s)$$ 并不是已经兑现的一条轨迹。它是一个期望值，描述了在状态 $$s$$ 之后，动作轨迹上的累积反馈奖励 $$G_t$$，它把接下来的 $$T$$ 步内的反馈奖励做了一个加权的求和。$$\\gamma$$ 是一个随着时间逐渐减退的影响因子，$$\\gamma\xa0\\in\xa0(0, 1)$$，其含义为当前的策略更加关注就近的奖励回报，而不太在乎未来步骤的 reward。"}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"以马里奥游戏为例："}),"\n",(0,s.jsx)(e.p,{children:"轨迹：平坦的路 => 按前进键 => 前方有乌龟 => 按射击键 => 前方有陷阱 => 按跳跃键 => 营救公主 => 胜利"}),"\n",(0,s.jsx)(e.p,{children:"涉及三个状态：平坦的路、前方有乌龟、前方有陷阱。"}),"\n",(0,s.jsx)(e.p,{children:"涉及三个动作：按前进键、按射击键、按跳跃键。"}),"\n",(0,s.jsx)(e.p,{children:"营救公主，获取奖励值 1。"}),"\n",(0,s.jsx)(e.p,{children:"以此我们可以根据计算出 “平坦的路” 这个状态的价值。"}),"\n"]}),"\n",(0,s.jsxs)(e.h3,{id:"动作价值函数",children:["动作价值函数",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#动作价值函数",children:"#"})]}),"\n",(0,s.jsx)(e.p,{children:"与状态价值函数相类似，动作价值函数指在当前状态 $$s$$，执行动作 $$a$$ 之后，依旧遵循策略 $$\\pi$$ 可以获得的期望回报价值。"}),"\n",(0,s.jsx)(e.p,{children:"$$Q^{\\pi}(s,a)=E_{\\pi}[G_t|S_t=s, A_t=a]$$"}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"假设，小李的手上有 1 万份股票（状态），而股票的价格在未来会波动，其潜在的价值即未来可能的价格的平均值。所谓动作价值函数，即现在小李就把其中的 5000 份股票卖掉（动作），剩余 5000 份获取的潜在价值，当然，也包括小李当下卖掉 5000 份股票直接赚得的钱。"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"动作价值函数和状态价值函数之间有如下关联关系："}),"\n",(0,s.jsx)(e.p,{children:"$$Q^{\\pi}(s,a)=r(s,a) + \\gamma \\sum_{s' \\in S}\xa0P(s'|s,a)V^{\\pi}(s')$$"}),"\n",(0,s.jsx)(e.p,{children:"其中，$$s'$$ 指的是继 $$s$$ 状态之后的状态。动作价值函数，包含了当前动作带来的即时的反馈奖励 reward，以及接下来之后的潜在状态价值。"}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsxs)(e.p,{children:["在马里奥游戏中，依据当前状态选择策略，得到的下一状态实际上是确定的。当玩家遇到陷阱，并选择跳跃键后，100% 会成功越过陷阱。因此，在这种",(0,s.jsx)(e.strong,{children:"确定性策略"}),"下，两个价值函数的关系可以改为："]}),"\n",(0,s.jsx)(e.p,{children:"$$Q^{\\pi}(s,a)=r(s,a) + \\gamma V^{\\pi}(s')$$"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"当然，强化学习的内涵和外延远远超出上述讲解的基本概念范畴，我们只需要厘清强化学习的本质和思路，方便后续理解 ChatGPT 所采用的做法就可以了。"}),"\n",(0,s.jsxs)(e.h1,{id:"强化学习与-nlp-相结合的困难点",children:["强化学习与 NLP 相结合的困难点",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#强化学习与-nlp-相结合的困难点",children:"#"})]}),"\n",(0,s.jsx)(e.p,{children:"强化学习最早主要应用在围棋等棋牌游戏以及各种大型电子游戏里，比如，王者荣耀里就有基于强化学习的 AI 人机模式。"}),"\n",(0,s.jsx)(e.p,{children:"强化学习较容易应用在这些场景中，原因就是这些场景都是人为构造的虚拟环境，最终的达成目标也较为简单。换句话说，针对游戏来说，环境容易创造，reward 容易构造。"}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"对于超级马里奥游戏而言，游戏软件创造的所有闯关关卡就是其强化学习的环境，而最后的 reward 值是多少，游戏软件会即刻给出玩家是通关或失败。"}),"\n",(0,s.jsx)(e.p,{children:"对于 AlphaGo 而言，环境就是围棋，围棋棋盘就是它的整个世界，而最后的 reward 值就是判断下棋双方哪一方胜利了，这对计算机来讲就是执行一个程序而已。"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"而想要在 NLP 上使用强化学习，那可就太难了。"}),"\n",(0,s.jsxs)(e.p,{children:["因为自然语言本质上是一种描述世界的渠道。整个现实世界的事物都可以通过自然语言来表示，形成抽象概念和抽象关系。",(0,s.jsx)(e.strong,{children:"因此，NLP 所依赖的环境是整个现实世界"}),"，整个世界的复杂度远远不是一个 19 乘 19 的棋盘可以比拟的。同时，NLP 领域无法设计 reward 函数。在 ChatGPT 创造出来之前，没有任何一个计算机程序，能够对一个 NLP 程序输出的结果给出准确的优劣判断。在 NLP 领域，reward 值只能由人工一个个给出。"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"为 NLP 模型构建 reward 函数，有点像鸡生蛋、蛋生鸡的死循环。"}),"\n",(0,s.jsx)(e.p,{children:"我们希望能够通过强化学习的方式，制作出一个先进的、通过图灵测试的语言模型。"}),"\n",(0,s.jsx)(e.p,{children:"为了实现强化学习，我们需要一个先进的、通过图灵测试的语言模型来做 reward。"}),"\n"]}),"\n",(0,s.jsxs)(e.h1,{id:"chatgpt-与强化学习",children:["ChatGPT 与强化学习",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#chatgpt-与强化学习",children:"#"})]}),"\n",(0,s.jsx)(e.p,{children:"介绍完强化学习的基本概念和建模形式之后，我们用两个例子（马里奥和围棋）介绍了一下强化学习如何应用在具体的任务中。大家可以举一反三，试想一下，强化学习应该如何应用在自然语言处理中。"}),"\n",(0,s.jsx)(e.p,{children:"在围棋中，棋盘就是环境，有很多电子围棋程序能够自动计算出下棋双方，哪一方胜利，哪一方失败。马里奥游戏同理，判断游戏失败的条件非常简单，碰到危险物体，掉进沟里，只需要写一段简单的程序即可。根据强化学习“吃一堑长一智”的学习原则，只有当智能体接收到胜利或失败的反馈信息后，才能够收获智能。"}),"\n",(0,s.jsx)(e.p,{children:"而在自然语言中，ChatGPT 作为一个智能体，其模型的优化也必须建立在，有一个程序或人，来告诉 ChatGPT，你模型给我生成的输出内容究竟是好，还是不好。若是生成的不好，那么 ChatGPT 就拿着这个负反馈回炉重造，以此达到“吃一堑长一智”的学习原则。"}),"\n",(0,s.jsxs)(e.h2,{id:"chatgpt-的强化学习概念映射",children:["ChatGPT 的强化学习概念映射",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#chatgpt-的强化学习概念映射",children:"#"})]}),"\n",(0,s.jsx)(e.p,{children:"我们先来回顾一下 ChatGPT 的工作流程："}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)("img",{src:h,alt:"10-3.png"})}),"\n",(0,s.jsx)(e.p,{children:"在上文的强化学习介绍中，我们以马里奥游戏为例进行介绍，如何闯关，如何胜利。其中涉及一个时间的概念，即马里奥的前进过程中，始终有一个状态随时间变化的问题。"}),"\n",(0,s.jsx)(e.p,{children:"而 ChatGPT 模型建模上并没有时间这个概念，因此，ChatGPT 以强化学习形式进行建模，对应的强化学习要素有如下变化："}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:"智能体：即 ChatGPT 模型本身；"}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:"环境：整个现实世界，被自然语言所描述和抽象，ChatGPT 实际是和人交互，因此，对于 ChatGPT 而言，人类用户就是它的环境；"}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:"状态：在 ChatGPT 当中，根据 ChatGPT 输入输出流程图可知，状态就是 prompt。此时，状态不再依赖时间存在（像马里奥游戏那样），时间被省略掉了，建模形式是无时间状态的；ChatGPT 实际上只关注输入和输出，也就是，我们只关心这一次的反馈就好，不需要像马里奥游戏一样再关心下次动作和反馈；"}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:"策略、动作：我们知道，强化学习中策略（Policy）实际上就是一个对环境响应的概率分布，而 ChatGPT 本身就是一个大语言模型概率分布（第 3 节中介绍）。因此，ChatGPT 模型对给定的输入，反馈一条输出文本，就是按照当前模型策略，进行了一次动作；"}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:"反馈 reward：人对于 ChatGPT 输出结果的评价，好或者差；注意，在上一节中，我们提到了，人工标注数据的代价非常大。这也是 reward 难以制做之处。"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.h2,{id:"制作-reward-model",children:["制作 reward model",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#制作-reward-model",children:"#"})]}),"\n",(0,s.jsxs)(e.p,{children:["ChatGPT 应用强化学习的主要困难，就是",(0,s.jsx)(e.strong,{children:"模型给出的输出反馈，没有什么方便的程序或者机制给出恰当的评价，就只能靠人工一个个的反馈"}),"。"]}),"\n",(0,s.jsx)(e.p,{children:"OpenAI 还是财大气粗，愿意花钱做这些看起来没技术含量的工作，公司找了 40 个外包，标注了大量的数据。利用这些标注数据，制作了一个 reward model，一举解决了设计奖励函数的问题。"}),"\n",(0,s.jsxs)(e.h1,{id:"总结",children:["总结",(0,s.jsx)(e.a,{className:"header-anchor","aria-hidden":"true",href:"#总结",children:"#"})]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"强化学习是让智能体（Agent，即人工智能模型）通过与环境的交互来学习如何做出最优决策（Policy）。"}),"\n",(0,s.jsx)(e.li,{children:"NLP 与强化学习结合的难点在于环境复杂（现实世界事物无限多），reward 函数难设计（只能靠人工评价模型输出的好坏）。"}),"\n"]})]})}function t(){let n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:e}=Object.assign({},(0,i.ah)(),n.components);return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(a,{...n})}):a(n)}let x=t;t.__RSPRESS_PAGE_META={},t.__RSPRESS_PAGE_META["%E4%BA%BA%E4%BA%BA%E9%83%BD%E8%83%BD%E7%9C%8B%E6%87%82%E7%9A%84%20ChatGPT%20%E5%8E%9F%E7%90%86%E8%AF%BE%2F10.%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%9F%BA%E7%A1%80%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9F.md"]={toc:[{text:"强化学习要素",id:"强化学习要素",depth:2},{text:"一条强化学习路径",id:"一条强化学习路径",depth:2},{text:"价值函数",id:"价值函数",depth:2},{text:"状态价值函数",id:"状态价值函数",depth:3},{text:"动作价值函数",id:"动作价值函数",depth:3},{text:"ChatGPT 的强化学习概念映射",id:"chatgpt-的强化学习概念映射",depth:2},{text:"制作 reward model",id:"制作-reward-model",depth:2}],title:"总结",headingTitle:"总结",frontmatter:{}}}}]);